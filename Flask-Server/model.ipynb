{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e92VNkGBr9l",
        "outputId": "e71f0d3f-3c2a-4ed7-9579-005bc649ad4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Specify the path to the ZIP file containing the images\n",
        "zip_file_path = '/content/drive/MyDrive/Photos_Walking Audit-20230701T115400Z-001.zip'\n",
        "\n",
        "# Specify the directory where you want to extract the images\n",
        "extracted_images_dir = '/content/extracted_images'\n",
        "\n",
        "# Extract the images from the ZIP file to the specified directory\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_images_dir)\n",
        "\n",
        "# Get the path to the extracted image directory\n",
        "image_dir_path = os.path.abspath(extracted_images_dir)\n",
        "\n",
        "# Print the path to the image directory\n",
        "print(\"Path to the extracted image directory:\", image_dir_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHCLRR85oV2o",
        "outputId": "1e63f59c-e799-40ae-e0f1-2bc64d174456"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to the extracted image directory: /content/extracted_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "data = pd.read_excel('/content/Issues_Collected_category and issue.xlsx')\n"
      ],
      "metadata": {
        "id": "AiN5G3t1lkS5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_names = data['Photo']\n",
        "l={}\n",
        "for i in range(0,len(image_file_names)):\n",
        "  l[image_file_names[i]] = data.iloc[i]['Issue type']"
      ],
      "metadata": {
        "id": "66DnJGBLllmN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "label_encoder = OneHotEncoder()\n",
        "l1 = list(set(l.values()))\n",
        "print(l1)\n",
        "\n",
        "\n",
        "# Reshape the label values to a 2D array-like shape\n",
        "l1_2d = [[value] for value in l1]\n",
        "\n",
        "# Fit and transform the reshaped labels\n",
        "labels_encoded = label_encoder.fit_transform(l1_2d).toarray()\n",
        "\n",
        "# Create a dictionary to map the encoded labels to their original values\n",
        "encoded_labels = {}\n",
        "for i, label in enumerate(l1):\n",
        "    encoded_labels[label] = labels_encoded[i]\n",
        "\n",
        "print(encoded_labels)\n",
        "print(encoded_labels.keys())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYyeMklcvoNF",
        "outputId": "b7bec7a4-da22-44c2-8e8f-a63d92e9ddd6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Encroachment by vendor or shop spillover', 'Transformers', 'Junction boxes', 'Silt accumulation in drains', 'Dark zone', 'Broken footpath', 'Garbage vulnerable point', 'Encroachment by planters', 'Encroachment by parking', 'Litter on street (Commercial)', 'Construction debris', 'Yellow spots', 'Silt accumulation on road', 'No footpath', 'Overgrown weeds', 'Low hanging wires', 'Level difference']\n",
            "{'Encroachment by vendor or shop spillover': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Transformers': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), 'Junction boxes': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Silt accumulation in drains': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), 'Dark zone': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Broken footpath': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Garbage vulnerable point': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Encroachment by planters': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Encroachment by parking': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Litter on street (Commercial)': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), 'Construction debris': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Yellow spots': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), 'Silt accumulation on road': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), 'No footpath': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), 'Overgrown weeds': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), 'Low hanging wires': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), 'Level difference': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
            "dict_keys(['Encroachment by vendor or shop spillover', 'Transformers', 'Junction boxes', 'Silt accumulation in drains', 'Dark zone', 'Broken footpath', 'Garbage vulnerable point', 'Encroachment by planters', 'Encroachment by parking', 'Litter on street (Commercial)', 'Construction debris', 'Yellow spots', 'Silt accumulation on road', 'No footpath', 'Overgrown weeds', 'Low hanging wires', 'Level difference'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define the image dimensions\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Create empty lists to store the preprocessed images and labels\n",
        "preprocessed_images = []\n",
        "preprocessed_labels = []\n",
        "i=0\n",
        "# Iterate over the image file names\n",
        "for file_name in image_file_names:\n",
        "    # print(file_name)\n",
        "    # Load the image using OpenCV or PIL\n",
        "    file_path = os.path.join('/content/extracted_images/Photos_Walking Audit/', file_name)\n",
        "    img = cv2.imread('/content/extracted_images/Photos_Walking Audit/' + file_name)  # Update the path accordingly\n",
        "    # print(img)\n",
        "\n",
        "    # Preprocess the image (resize, normalize, etc.)\n",
        "    # try:\n",
        "    if img is not None:\n",
        "      img = cv2.resize(img, (img_width, img_height))\n",
        "      img = img.astype('float32') / 255.0\n",
        "\n",
        "    # Add the preprocessed image and corresponding label to the lists\n",
        "      preprocessed_images.append(img)\n",
        "      preprocessed_labels.append(encoded_labels[l[file_name]])\n",
        "\n",
        "\n",
        "\n",
        "# Convert the lists to NumPy arrays\n",
        "preprocessed_images = np.array(preprocessed_images)\n",
        "preprocessed_labels = np.array(preprocessed_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "tloaMwtLlqh9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the dataset into train, validation, and test sets\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(preprocessed_images,preprocessed_labels, test_size=0.1, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.2, random_state=42)\n",
        "print(len(X_train))\n",
        "\n",
        "\n",
        "print(y_train[0])\n"
      ],
      "metadata": {
        "id": "3rESJnJhlrTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a023472a-6f2c-44d8-97de-ae3953405f9e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "540\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, BatchNormalization, Dense, MaxPooling2D, Activation,Dropout"
      ],
      "metadata": {
        "id": "bXRCKphQ3_NI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 17\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "# # Load pre-trained VGG16 model (excluding the fully connected layers)\n",
        "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# # Freeze the base model layers to prevent them from being updated during training\n",
        "# base_model.trainable = False\n",
        "\n",
        "# # Create a new model\n",
        "# model = Sequential()\n",
        "\n",
        "# # Add the pre-trained VGG16 base model to the new model\n",
        "# model.add(base_model)\n",
        "\n",
        "# # Add custom fully connected layers on top of the pre-trained model\n",
        "# model.add(Flatten())\n",
        "# model.add()\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(512, activation='relu'))\n",
        "# model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "# Load pre-trained VGG16 model (excluding the fully connected layers)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers to prevent them from being updated during training\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the pre-trained VGG16 base model to the new model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add additional Conv2D layers\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "# Add custom fully connected layers on top of the pre-trained model\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuM-htKCuito",
        "outputId": "3e249d0b-0880-4ef0-d49a-29ee1a2e892c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 7, 7, 128)         589952    \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 7, 7, 512)         1180160   \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 256)               6422784   \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 17)                8721      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,343,057\n",
            "Trainable params: 8,628,369\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "17/17 [==============================] - 21s 798ms/step - loss: 2.9772 - accuracy: 0.1167 - val_loss: 2.5314 - val_accuracy: 0.1042\n",
            "Epoch 2/20\n",
            "17/17 [==============================] - 3s 171ms/step - loss: 2.5059 - accuracy: 0.1648 - val_loss: 2.2351 - val_accuracy: 0.3125\n",
            "Epoch 3/20\n",
            "17/17 [==============================] - 3s 173ms/step - loss: 2.2288 - accuracy: 0.2370 - val_loss: 2.0765 - val_accuracy: 0.3333\n",
            "Epoch 4/20\n",
            "17/17 [==============================] - 3s 173ms/step - loss: 2.0603 - accuracy: 0.2870 - val_loss: 2.2395 - val_accuracy: 0.2917\n",
            "Epoch 5/20\n",
            "17/17 [==============================] - 3s 172ms/step - loss: 2.0123 - accuracy: 0.3148 - val_loss: 2.0538 - val_accuracy: 0.2292\n",
            "Epoch 6/20\n",
            "17/17 [==============================] - 3s 170ms/step - loss: 1.7625 - accuracy: 0.3870 - val_loss: 2.0366 - val_accuracy: 0.2917\n",
            "Epoch 7/20\n",
            "17/17 [==============================] - 3s 170ms/step - loss: 1.6002 - accuracy: 0.4130 - val_loss: 2.1524 - val_accuracy: 0.3125\n",
            "Epoch 8/20\n",
            "17/17 [==============================] - 3s 178ms/step - loss: 1.3870 - accuracy: 0.5000 - val_loss: 2.2258 - val_accuracy: 0.3958\n",
            "Epoch 9/20\n",
            "17/17 [==============================] - 3s 173ms/step - loss: 1.1615 - accuracy: 0.5759 - val_loss: 2.7510 - val_accuracy: 0.3333\n",
            "Epoch 10/20\n",
            "17/17 [==============================] - 3s 171ms/step - loss: 1.0458 - accuracy: 0.6389 - val_loss: 2.1972 - val_accuracy: 0.3333\n",
            "Epoch 11/20\n",
            "17/17 [==============================] - 3s 170ms/step - loss: 0.8532 - accuracy: 0.7074 - val_loss: 2.8083 - val_accuracy: 0.2917\n",
            "Epoch 12/20\n",
            "17/17 [==============================] - 3s 172ms/step - loss: 0.7887 - accuracy: 0.7204 - val_loss: 3.3169 - val_accuracy: 0.3958\n",
            "Epoch 13/20\n",
            "17/17 [==============================] - 3s 174ms/step - loss: 0.5999 - accuracy: 0.7926 - val_loss: 3.2368 - val_accuracy: 0.4167\n",
            "Epoch 14/20\n",
            "17/17 [==============================] - 3s 161ms/step - loss: 0.5696 - accuracy: 0.8167 - val_loss: 3.1927 - val_accuracy: 0.3958\n",
            "Epoch 15/20\n",
            "17/17 [==============================] - 3s 166ms/step - loss: 0.4372 - accuracy: 0.8593 - val_loss: 2.9658 - val_accuracy: 0.4375\n",
            "Epoch 16/20\n",
            "17/17 [==============================] - 3s 161ms/step - loss: 0.3654 - accuracy: 0.8852 - val_loss: 3.6693 - val_accuracy: 0.4167\n",
            "Epoch 17/20\n",
            "17/17 [==============================] - 3s 168ms/step - loss: 0.2092 - accuracy: 0.9315 - val_loss: 3.9750 - val_accuracy: 0.4375\n",
            "Epoch 18/20\n",
            "17/17 [==============================] - 3s 170ms/step - loss: 0.1428 - accuracy: 0.9500 - val_loss: 5.2130 - val_accuracy: 0.2917\n",
            "Epoch 19/20\n",
            "17/17 [==============================] - 3s 166ms/step - loss: 0.1429 - accuracy: 0.9426 - val_loss: 4.2521 - val_accuracy: 0.3958\n",
            "Epoch 20/20\n",
            "17/17 [==============================] - 3s 166ms/step - loss: 0.1485 - accuracy: 0.9574 - val_loss: 5.0437 - val_accuracy: 0.3125\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1208 - accuracy: 0.5000\n",
            "Test Loss: 2.1208415031433105\n",
            "Test Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model.save('/content/drive/MyDrive/my_models/final_pavement.h5')"
      ],
      "metadata": {
        "id": "Ge3PjK6YQgRA"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}